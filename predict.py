# coding:utf-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import re
import os
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# ===================== 1. åŸºç¡€é…ç½®ä¸é¢„å¤„ç†å‡½æ•°ï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰ =====================
# ä¸‹è½½å¿…è¦çš„NLTKèµ„æºï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)

# å›ºå®šéšæœºç§å­
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# è®¾å¤‡é…ç½®ï¼šä¼˜å…ˆGPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device for inference: {device}")

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    # æ¸…æ´—æ–‡æœ¬ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€æ•°å­—ï¼Œè½¬å°å†™
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A)
    text = text.lower().strip()
    # åˆ†è¯ + å»åœç”¨è¯ + è¯å¹²æå–
    tokens = nltk.word_tokenize(text)
    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words and len(token) > 2]
    return tokens

# åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„è¯æ±‡è¡¨ï¼ˆæ ¸å¿ƒï¼‰
def load_vocab(vocab_path='vocab.txt'):
    """
    åŠ è½½è®­ç»ƒä»£ç ç”Ÿæˆçš„vocab.txt
    :param vocab_path: è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
    :return: è¯æ±‡è¡¨å­—å…¸ã€PADç´¢å¼•ã€UNKç´¢å¼•
    """
    if not os.path.exists(vocab_path):
        raise FileNotFoundError(f"Vocabulary file {vocab_path} not found! Please run training code first.")
    
    vocab = {}
    with open(vocab_path, 'r', encoding='utf-8') as f:
        for line in f:
            token, idx = line.strip().split('\t')
            vocab[token] = int(idx)
    
    # è·å–ç‰¹æ®Šæ ‡è®°çš„ç´¢å¼•ï¼ˆè®­ç»ƒæ—¶å›ºå®šï¼š<pad>=0, <unk>=1ï¼‰
    pad_idx = vocab.get('<pad>', 0)
    unk_idx = vocab.get('<unk>', 1)
    
    print(f"âœ… Vocabulary loaded from {vocab_path}, size: {len(vocab)}")
    print(f"  - <pad> index: {pad_idx}")
    print(f"  - <unk> index: {unk_idx}")
    return vocab, pad_idx, unk_idx

# æ–‡æœ¬è½¬æ¨¡å‹è¾“å…¥å¼ é‡ï¼ˆä¸è®­ç»ƒæ—¶çš„å¤„ç†é€»è¾‘ä¸€è‡´ï¼‰
def text_to_tensor(text, vocab, max_len=500):
    """
    å°†åŸå§‹æ–‡æœ¬è½¬ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„å¼ é‡
    :param text: è¾“å…¥è‹±æ–‡æ–‡æœ¬
    :param vocab: åŠ è½½çš„è¯æ±‡è¡¨
    :param max_len: æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼Œé»˜è®¤500ï¼‰
    :return: æ¨¡å‹è¾“å…¥å¼ é‡ [1, max_len]ã€é¢„å¤„ç†åçš„tokens
    """
    # é¢„å¤„ç†æ–‡æœ¬ï¼ˆå’Œè®­ç»ƒæ—¶ç›¸åŒçš„é€»è¾‘ï¼‰
    tokens = preprocess_text(text)
    # è½¬ç´¢å¼•åºåˆ—ï¼ˆæœªçŸ¥è¯æ˜ å°„ä¸º<unk>ï¼‰
    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]
    # å›ºå®šé•¿åº¦ï¼šè¡¥PAD/æˆªæ–­
    if len(indices) < max_len:
        indices += [vocab['<pad>']] * (max_len - len(indices))
    else:
        indices = indices[:max_len]
    # è½¬ä¸ºå¼ é‡å¹¶æ·»åŠ batchç»´åº¦ [max_len] â†’ [1, max_len]
    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)
    return tensor, tokens

# ===================== 2. æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰ =====================
# TextCNNæ¨¡å‹
class TextCNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
        self.convs = nn.ModuleList([
            nn.Conv2d(in_channels=1, out_channels=n_filters, 
                      kernel_size=(fs, embedding_dim)) for fs in filter_sizes
        ])
        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        embedded = self.embedding(text).unsqueeze(1)  # [batch, 1, seq_len, emb_dim]
        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # [batch, n_filters, seq_len-fs+1]
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # [batch, n_filters]
        cat = self.dropout(torch.cat(pooled, dim=1))  # [batch, n_filters * len(filter_sizes)]
        return torch.sigmoid(self.fc(cat))

# TextLSTMæ¨¡å‹
class TextLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, 
                            bidirectional=bidirectional, dropout=dropout if n_layers>1 else 0)
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        text = text.permute(1, 0)  # [seq_len, batch_size]
        embedded = self.dropout(self.embedding(text))  # [seq_len, batch_size, emb_dim]
        output, (hidden, cell) = self.lstm(embedded)
        
        if self.lstm.bidirectional:
            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))
        else:
            hidden = self.dropout(hidden[-1,:,:])
        
        return torch.sigmoid(self.fc(hidden))

# ===================== 3. æ¨¡å‹åŠ è½½ä¸æ¨ç†æ ¸å¿ƒå‡½æ•° =====================
def load_trained_model(model_type, vocab_size, model_path, pad_idx):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆè¶…å‚æ•°ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰
    :param model_type: 'cnn' æˆ– 'lstm'
    :param vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆä»åŠ è½½çš„vocabè·å–ï¼‰
    :param model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚TextCNN_best.ptï¼‰
    :param pad_idx: PADæ ‡è®°çš„ç´¢å¼•
    :return: åŠ è½½å¥½çš„æ¨¡å‹ï¼ˆevalæ¨¡å¼ï¼‰
    """
    # è¶…å‚æ•°ï¼ˆå¿…é¡»ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰
    EMBEDDING_DIM = 100
    OUTPUT_DIM = 1
    DROPOUT = 0.5

    # åŠ è½½å¯¹åº”æ¨¡å‹
    if model_type.lower() == 'cnn':
        N_FILTERS = 100
        FILTER_SIZES = [3, 4, 5]
        model = TextCNN(
            vocab_size=vocab_size,
            embedding_dim=EMBEDDING_DIM,
            n_filters=N_FILTERS,
            filter_sizes=FILTER_SIZES,
            output_dim=OUTPUT_DIM,
            dropout=DROPOUT,
            pad_idx=pad_idx
        ).to(device)
    elif model_type.lower() == 'lstm':
        HIDDEN_DIM = 128
        N_LAYERS = 2
        BIDIRECTIONAL = True
        model = TextLSTM(
            vocab_size=vocab_size,
            embedding_dim=EMBEDDING_DIM,
            hidden_dim=HIDDEN_DIM,
            output_dim=OUTPUT_DIM,
            n_layers=N_LAYERS,
            bidirectional=BIDIRECTIONAL,
            dropout=DROPOUT,
            pad_idx=pad_idx
        ).to(device)
    else:
        raise ValueError("model_type must be 'cnn' or 'lstm'")

    # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå…¼å®¹CPU/GPUï¼‰
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file {model_path} not found! Please check the path.")
    
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­Dropoutï¼‰
    print(f"âœ… {model_type.upper()} model loaded from {model_path}")
    return model

def predict_sentiment(text, model, vocab, max_len=500):
    """
    å•æ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹æ ¸å¿ƒå‡½æ•°
    :param text: è¾“å…¥çš„è‹±æ–‡æ–‡æœ¬
    :param model: åŠ è½½å¥½çš„æ¨¡å‹
    :param vocab: åŠ è½½çš„è¯æ±‡è¡¨
    :param max_len: æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
    :return: åŒ…å«è¯¦ç»†ä¿¡æ¯çš„é¢„æµ‹ç»“æœå­—å…¸
    """
    # æ–‡æœ¬è½¬å¼ é‡
    tensor, tokens = text_to_tensor(text, vocab, max_len)
    
    # æ¨ç†ï¼ˆç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œæå‡é€Ÿåº¦ï¼‰
    with torch.no_grad():
        pred = model(tensor).squeeze(0)  # [1,1] â†’ [1]
        confidence = pred.item()  # ç½®ä¿¡åº¦ï¼ˆ0~1ï¼Œè¶Šæ¥è¿‘1è¶Šæ­£é¢ï¼Œè¶Šæ¥è¿‘0è¶Šè´Ÿé¢ï¼‰
        sentiment = "Positive" if confidence >= 0.5 else "Negative"
    
    # è¿”å›ç»“æ„åŒ–ç»“æœ
    return {
        "input_text": text,
        "processed_tokens": tokens,  # é¢„å¤„ç†åçš„tokensï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        "sentiment": sentiment,      # æƒ…æ„Ÿæ ‡ç­¾ï¼ˆPositive/Negativeï¼‰
        "confidence": round(confidence, 4),  # ç½®ä¿¡åº¦ï¼ˆä¿ç•™4ä½å°æ•°ï¼‰
        "confidence_interpretation": f"{confidence*100:.2f}% {sentiment.lower()}"  # å¯è¯»æ€§è§£é‡Š
    }

def batch_predict(texts, model, vocab, max_len=500):
    """
    æ‰¹é‡æ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹
    :param texts: æ–‡æœ¬åˆ—è¡¨
    :return: é¢„æµ‹ç»“æœåˆ—è¡¨
    """
    results = []
    print(f"\nğŸ“ Starting batch prediction for {len(texts)} texts...")
    for i, text in enumerate(texts):
        result = predict_sentiment(text, model, vocab, max_len)
        result["sample_id"] = i + 1  # æ·»åŠ æ ·æœ¬IDï¼Œä¾¿äºåŒºåˆ†
        results.append(result)
        # æ‰“å°è¿›åº¦
        if (i + 1) % 5 == 0 or (i + 1) == len(texts):
            print(f"  - Processed {i + 1}/{len(texts)} samples")
    return results

# ===================== 4. ç»“æœæ‰“å°è¾…åŠ©å‡½æ•° =====================
def print_prediction_result(result):
    """
    ç¾è§‚æ‰“å°å•æ¡é¢„æµ‹ç»“æœ
    """
    print("\n" + "-"*80)
    print(f"Sample ID: {result.get('sample_id', 1)}")
    print(f"Input Text: {result['input_text'][:100]}..." if len(result['input_text'])>100 else f"Input Text: {result['input_text']}")
    print(f"Processed Tokens: {', '.join(result['processed_tokens'])[:100]}..." if len(result['processed_tokens'])>10 else f"Processed Tokens: {', '.join(result['processed_tokens'])}")
    print(f"Predicted Sentiment: {result['sentiment']}")
    print(f"Confidence: {result['confidence_interpretation']}")
    print("-"*80)

# ===================== 5. ä¸»æ¨ç†æµç¨‹ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰ =====================
if __name__ == '__main__':
    # -------------------------- é…ç½®é¡¹ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰ --------------------------
    MODEL_TYPE = "cnn"  # å¯é€‰ï¼š'cnn' æˆ– 'lstm'
    MODEL_PATH = "TextCNN_best.pt"  # è®­ç»ƒç”Ÿæˆçš„æ¨¡å‹æƒé‡è·¯å¾„
    VOCAB_PATH = "vocab.txt"  # è®­ç»ƒç”Ÿæˆçš„è¯æ±‡è¡¨è·¯å¾„
    MAX_LEN = 500  # å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
    
    # -------------------------- æ­¥éª¤1ï¼šåŠ è½½è¯æ±‡è¡¨ --------------------------
    try:
        vocab, pad_idx, unk_idx = load_vocab(VOCAB_PATH)
        vocab_size = len(vocab)
    except Exception as e:
        print(f"âŒ Error loading vocabulary: {e}")
        exit()
    
    # -------------------------- æ­¥éª¤2ï¼šåŠ è½½æ¨¡å‹ --------------------------
    try:
        model = load_trained_model(MODEL_TYPE, vocab_size, MODEL_PATH, pad_idx)
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        exit()
    
    # -------------------------- æ­¥éª¤3ï¼šå•æ–‡æœ¬æ¨ç†ç¤ºä¾‹ --------------------------
    print("\n=== Single Text Inference Example ===")
    # æµ‹è¯•æ–‡æœ¬1ï¼ˆæ­£é¢ï¼‰
    test_text1 = "This movie is absolutely fantastic! The acting is brilliant and the plot is so engaging. I would watch it again and again."
    result1 = predict_sentiment(test_text1, model, vocab, MAX_LEN)
    print_prediction_result(result1)
    
    # æµ‹è¯•æ–‡æœ¬2ï¼ˆè´Ÿé¢ï¼‰
    test_text2 = "Worst movie I've ever seen! The story is boring, the characters are unlikable, and the ending is terrible. Total waste of time."
    result2 = predict_sentiment(test_text2, model, vocab, MAX_LEN)
    print_prediction_result(result2)
    
    # æµ‹è¯•æ–‡æœ¬3ï¼ˆä¸­æ€§åæ­£é¢ï¼‰
    test_text3 = "The film was okay, not great but not terrible either. The cinematography was impressive and the soundtrack was nice."
    result3 = predict_sentiment(test_text3, model, vocab, MAX_LEN)
    print_prediction_result(result3)
    
    # -------------------------- æ­¥éª¤4ï¼šæ‰¹é‡æ¨ç†ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰ --------------------------
    print("\n=== Batch Inference Example ===")
    batch_texts = [
        "Amazing cinematography and a touching story, highly recommended!",
        "I wasted 2 hours of my life on this garbage film.",
        "The movie had a slow start but the second half was really good.",
        "Terrible acting and a confusing plot, I regret watching it.",
        "One of the best movies I've seen this year, 10/10!"
    ]
    batch_results = batch_predict(batch_texts, model, vocab, MAX_LEN)
    
    # æ‰“å°æ‰¹é‡ç»“æœ
    for res in batch_results:
        print_prediction_result(res)
    
    # -------------------------- æœ€ç»ˆæç¤º --------------------------
    print("\nğŸ‰ Inference completed successfully!")
    print("ğŸ’¡ Tips:")
    print("  - Modify the 'test_text' or 'batch_texts' list to predict your own texts")
    print("  - Ensure MODEL_TYPE, MODEL_PATH, VOCAB_PATH match your training output")
    print("  - Confidence > 0.5 = Positive, < 0.5 = Negative")